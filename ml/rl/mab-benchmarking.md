# MAB Algorithm Benchmarking

## 0. ê°œìš”

### Objective

* MAB ê¸°ë³¸ ì•Œê³ ë¦¬ì¦˜ë“¤ì˜ ì„±ëŠ¥\(convergence ì†ë„, latency\) ì¸¡ì •

### **ë°©ë²•**

* convergence: experimentsì˜ í‰ê·  rewardê°’ê³¼ ìµœì ì˜ armì„ ì„ íƒí•  í™•ë¥ ì„ plotting
* latency: agent ì´ˆê¸°í™” ë¶€ë¶„ì„ ì œì™¸í•œ ì‹¤ì œ MAB ì•Œê³ ë¦¬ì¦˜ êµ¬ë™ë¶€ì˜ ì†ë„ë§Œ ì¸¡ì •

```text
%matplotlib inline
import os
import sys
module_path = os.path.abspath(os.path.join('..'))
if module_path not in sys.path:
    sys.path.append(module_path)

import bandits as bd
from bandits import kullback
```

## **1. í•˜ì´í¼íŒŒë¼ë©”í„° ì„¤ì •**

* **Number of Arms**: armì˜ ê°¯ìˆ˜, ì¦‰ k
* **Number of Trials**: Number of Iterations \(Time steps\)
* **Number of Experiments**: Converge ì²™ë„ ì‹¤í—˜; ê°ì ë‹¤ë¥¸ ì„¸íŒ…ì˜\(default:random\) 500ê°œì˜ k armsë“¤ì„ êµ¬ë™ì‹œì¼œ í‰ê· ì„ ì·¨í•˜ê¸° ìœ„í•œ ëª©ì ì„

```text
n_arms = 10
n_trials = 1000
n_experiments = 200
```

## **2. ğœ–-greedy\(epsilon-greedy\)**

* ê°œìš”: ğœ– í™•ë£°ë¡œ randomí•˜ê²Œ íƒìƒ‰í•˜ê³  1 âˆ’ ğœ–ì˜ í™•ë¥ ë¡œ greedyí•˜ê²Œ ê°€ì¥ ì¢‹ì€ armì„ ì„ íƒí•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ğœ–ì´ í¬ë©´ íƒìƒ‰ì˜ ë¹„ì¤‘ì´ ë†’ì•„ì§€ê³  ğœ–ì´ ì‘ìœ¼ë©´ íšë“ì˜ ë¹„ì¤‘ì´ ë†’ì•„ì§
* ë¹„êµ ì•Œê³ ë¦¬ì¦˜
  * Random: ğœ–=1 ì¸ ê²½ìš°
  * Greedy: ğœ–=0 ì¸ ê²½ìš°
  * EpsilonGreedy \(w/ ğœ–=0.01\): ğœ–=0.01 ì¸ ê²½ìš°
  * EpsilonGreedy \(w/ ğœ–=0.1\): ğœ–=0.01 ì¸ ê²½ìš° \(ì¼ë°˜ì ìœ¼ë¡œ ì“°ì´ëŠ” ì„¸íŒ…ê°’ì´ë‚˜ í™˜ê²½ì— ë”°ë¼ ì¡°ì ˆ í•„ìš”\)
* ì‹¤í—˜ ê²°ê³¼ \(Converge Ratio\):
  * Random: Average Rewardì™€ Optimal Actionì˜ ì„ íƒ í™•ë¥ ì´ ë§¤ìš° ë‚®ìŒ
  * Greedy: Average Rewardê°€ Random ëŒ€ë¹„ ë†’ìœ¼ë‚˜ Optimal Actionì˜ ì„ íƒ í™•ë¥ ì´ ì¼ì • ì‹œì  ì´í›„ì— ì¦ê°€í•˜ì§€ ì•ŠìŒ
  * EpsilonGreedy \(w/ ğœ–=0.01\): Optimal Actionì˜ ì„ íƒ í™•ë¥ ì´ ì ì§„ì ìœ¼ë¡œ ì¦ê°€í•˜ì§€ë§Œ ìˆ˜ë ´ ì†ë„ê°€ ë”ë”¤
  * EpsilonGreedy \(w/ ğœ–=0.1\): Optimal Actionì˜ ì„ íƒ í™•ë¥ ì´ ë‚˜ë¨¸ì§€ ì…‹ ëŒ€ë¹„ ê°€ì¥ ì¢‹ìœ¼ë©° ì¦ê°€ ì†ë„ë„ ë¹ ë¦„
* ì‹¤í—˜ ê²°ê³¼ \(Elapsed Time\):
  * 4ê°€ì§€ Policy ëª¨ë‘ í° ì°¨ì´ëŠ” ì—†ìœ¼ë‚˜ Randomì˜ ê²½ìš° ì†ë„ê°€ ì•½ê°„ ë–¨ì–´ì§

```text
bandit = bd.GaussianBandit(n_arms)
agents = [
    bd.Agent(bandit, bd.RandomPolicy()),
    bd.Agent(bandit, bd.GreedyPolicy()),
    bd.Agent(bandit, bd.EpsilonGreedyPolicy(0.01)),
    bd.Agent(bandit, bd.EpsilonGreedyPolicy(0.1)),
]
env = bd.Environment(bandit, agents, label='Epsilon Greedy')
scores, optimal = env.run(n_trials, n_experiments)
env.plot_results(scores, optimal)
```

![](../../.gitbook/assets/untitled%20%287%29.png)

## **3. UCB\(Upper Confidence Bound\)**

* ê°œìš”: Upper Confidence Bound\(ì´í•˜ UCB\)ë¥¼ ì„¤ì •í•˜ì—¬ rewardì— ëŒ€í•œ ë¶ˆí™•ì‹¤ì„±ì„ ê³ ë ¤
  * As-is: ì£¼ì‚¬ìœ„ë¥¼ ì—¬ëŸ¬ ë²ˆ ë˜ì¡Œì„ ë•Œ ê¸°ëŒ€ê°’ì€ 3.5ì´ì§€ë§Œ, ë‘ ë²ˆë§Œ ë˜ì¡Œì„ ë•Œ ëˆˆê¸ˆì´ 1, 3ì´ ë‚˜ì˜¤ë©´ ê¸°ëŒ€ê°’ì´ 2ê°€ ë‚˜ì˜¤ë¯€ë¡œ ì‹¤ì œ ê¸°ëŒ€ê°’ê³¼ í¸ì°¨ê°€ ì‹¬í•¨
  * To-be: ì£¼ì‚¬ìœ„ë¥¼ ë‘ ë²ˆë§Œ ë˜ì¡Œì„ ë•Œ \[2, 5.2\]ì˜ ë²”ìœ„ë¡œ Confidence Intervalì„ ì •í•˜ê³  íšŸìˆ˜ê°€ ì¦ê°€í•  ìˆ˜ë¡ Confidence Intervalì„ ê°ì†Œì‹œì¼œ ë¶ˆí™•ì‹¤ì„±ì„ ì¤„ì„
* ë¹„êµ ì•Œê³ ë¦¬ì¦˜
  * EpsilonGreedy \(w/ ğœ–=0.1\)
  * EpsilonGreedy \(w/ ğœ–=0.1, Initial Value=5\): Initial Valueë¥¼ ë†’ê²Œ ì„¤ì • ì‹œ ì´ˆê¸° Iterationì—ì„œ íƒìƒ‰ ë¹„ì¤‘ì´ ì˜¬ë¼ê°€ê¸° ë•Œë¬¸ì— ìµœì  Actionì˜ ì„ íƒ ë¹ˆë„ê°€ ëŠ˜ì–´ë‚¨
  * UCB \(w/ c\): cëŠ” Confidence levelë¥¼ ì¡°ì ˆí•˜ëŠ” í•˜ì´í¼íŒŒë¼ë©”í„°ë¡œ ê°’ì´ ë†’ì„ìˆ˜ë¡ íƒìƒ‰ ë¹ˆë„ ì¦ê°€
    * ì§ê´€ì  ì´í•´1: ê³„ì† íŠ¹ì • armë§Œ ì„ íƒí•˜ë©´ UCB termì´ ì‘ì•„ì§€ë¯€ë¡œ Confidence Intervalì´ ê°ì†Œí•˜ì—¬ íƒìƒ‰ ë¹ˆë„ ê°ì†Œ \(ğ‘ğ‘¡\(ğ‘\) ê°€ ì¦ê°€í•˜ë¯€ë¡œ\)
    * ì§ê´€ì  ì´í•´2: logì˜ ì—­í• ì€ UCB decayë¡œ ì ì°¨ Confidence Intervalì„ ê°ì†Œì‹œí‚¤ëŠ” ì—­í• 
    * ì§ê´€ì  ì´í•´3: ì˜ ì„ íƒë˜ì§€ ì•Šì€ armì€ Confidence Intervalì´ ê°ì†Œí•˜ì—¬ íƒìƒ‰ ë¹ˆë„ ì¦ê°€

### **3.1. Gaussian Bandits**

* ì‹¤í—˜ ê²°ê³¼ \(Converge Ratio\):
  * EpsilonGreedy \(w/ ğœ–=0.1, Initial Value=5\): Iteration ê·¹ì´ˆê¸°ì— % Optimal Action ì¦ê°€ ì†ë„ê°€ ì…‹ ì¤‘ ê°€ì¥ ë¹ ë¦„
  * UCB: Average Reward ë° Optimal Actionì˜ ì„ íƒ í™•ë¥ ì´ ì…‹ ì¤‘ ê°€ì¥ ë†’ì§€ë§Œ ì¤‘ê°„ì¤‘ê°„ ì„±ëŠ¥ì´ ì €í•˜ë˜ëŠ” êµ¬ê°„ ë°œìƒ
* ì‹¤í—˜ ê²°ê³¼ \(Elapsed Time\):
  * UCBì˜ ê²½ìš° ğœ–-greedy ëŒ€ë¹„ ì•½ 1.5ë°° ì •ë„ ì§€ì—°ë¨

```text
bandit = bd.GaussianBandit(n_arms)
agents = [
    bd.Agent(bandit, bd.EpsilonGreedyPolicy(0.1)),
    bd.Agent(bandit, bd.EpsilonGreedyPolicy(0.1), prior=5),
    bd.Agent(bandit, bd.UCBPolicy(1))
    #bd.Agent(bandit, bd.MOSSPolicy())    
]
env = bd.Environment(bandit, agents, label='Epsilon Greedy and UCB')
scores, optimal = env.run(n_trials, n_experiments)
env.plot_results(scores, optimal)
```



![](../../.gitbook/assets/untitled-1%20%287%29.png)

### 3.2. Bernoulli Bandits

```text
bandit = bd.BernoulliBanditNumpy(n_arms)
agents = [
    bd.Agent(bandit, bd.EpsilonGreedyPolicy(0.1)),
    bd.Agent(bandit, bd.EpsilonGreedyPolicy(0.1), prior=5),
    bd.Agent(bandit, bd.UCBPolicy(1))
]
env = bd.Environment(bandit, agents, label='Epsilon Greedy and UCB')
scores, optimal = env.run(n_trials, n_experiments)
env.plot_results(scores, optimal)
```

![](../../.gitbook/assets/untitled-2%20%284%29.png)

## **4. UCB Variants**

* ê°œìš”: ë‹¤ì–‘í•œ UCB ì•Œê³ ë¦¬ì¦˜ë“¤ì— ëŒ€í•œ ì„±ëŠ¥ í‰ê°€

### 4.1. Bernoulli Bandits

```text
bandit = bd.BernoulliBanditNumpy(n_arms)
agents = [
    bd.Agent(bandit, bd.UCBPolicy()),
    bd.Agent(bandit, bd.UCBVPolicy(2, 0.1)),
    bd.Agent(bandit, bd.UCBTunedPolicy()),
    bd.Agent(bandit, bd.KLUCBPolicy(klucb=kullback.klucbBern)),
    bd.Agent(bandit, bd.MOSSPolicy()),
    bd.BetaAgentUCB(bandit, bd.GreedyPolicy())
]
env = bd.Environment(bandit, agents, label='UCB Variants (Bernoulli Bandits)')
scores, optimal = env.run(n_trials, n_experiments)
env.plot_results(scores, optimal)
```

### 4.2. Binomial Bandits

```text
bandit = bd.BinomialBandit(n_arms, n=5, t=8*n_trials)
agents = [
    bd.Agent(bandit, bd.UCBPolicy()),
    bd.Agent(bandit, bd.UCBVPolicy(2, 0.1)),
    bd.Agent(bandit, bd.UCBTunedPolicy()),
    bd.Agent(bandit, bd.KLUCBPolicy(klucb=kullback.klucbBern)),
    bd.Agent(bandit, bd.MOSSPolicy()),
    bd.BetaAgentUCB(bandit, bd.GreedyPolicy())
]
env = bd.Environment(bandit, agents, label='UCB Variants (Binomial Bandits)')
scores, optimal = env.run(n_trials, n_experiments)
env.plot_results(scores, optimal)
```

## **5. TS\(Thompson Sampling\)**

* ê°œìš”: Beta ë¶„í¬ë¥¼ priorë¡œ ê°€ì •í•˜ê³  Banditì˜ ë¶„í¬ë¥¼ ë² ë¥´ëˆ„ì´ ë¶„í¬ë‚˜ ì´í•­ ë¶„í¬ì˜ í˜•íƒœë¥¼ ê°€ì§€ëŠ” likelihoodë¡œ ê°€ì •í•˜ì—¬ í™•ë£° ë¶„í¬ë¥¼ ëª¨ë¸ë§í•˜ëŠ” ë°©ë²•
* ë¹„êµ ì•Œê³ ë¦¬ì¦˜
  * EpsilonGreedy \(w/ ğœ–=0.1, Initial Value=5\)
  * UCB
  * TS

### **5.1. pymc3 ë¼ì´ë¸ŒëŸ¬ë¦¬**

* ì›ë³¸ ì†ŒìŠ¤ì½”ë“œë¡œ theanoì‚¬ìš©ìœ¼ë¡œ ì¸í•´ non-GPU í™˜ê²½ì—ì„œ ì†ë„ê°€ ëŠë¦¼
* ì‹¤í—˜ ê²°ê³¼ \(Converge Ratio\):
  * EpsilonGreedy \(w/ ğœ–=0.1, Initial Value=5\): ì•ˆì •ì ìœ¼ë¡œ ìˆ˜ë ´í•˜ì§€ë§Œ ì¼ì • Iteration ì´ìƒì—ì„œ ìˆ˜ë ´ì†ë„ê°€ ë–¨ì–´ì§
  * UCB: Average Rewardì™€ Optimal Actionì˜ ì„ íƒ í™•ë¥ ì´ Iterationì´ ë°˜ë³µë ìˆ˜ë¡ ê¾¸ì¤€íˆ ì¦ê°€í•˜ì§€ë§Œ ì¤‘ê°„ì¤‘ê°„ ì„±ëŠ¥ì´ ì €í•˜ë˜ëŠ” êµ¬ê°„ ë°œìƒ
  * TS: 1000ë²ˆ ì´ë‚´ì˜ time stepì—ì„œëŠ” ê°€ì¥ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ë‚˜ 5000íšŒ, 10000íšŒ ë“± long runìœ¼ë¡œ ê°ˆìˆ˜ë¡ UCBì— ë¹„í•´ ë¶ˆë¦¬í•  ìˆ˜ ìˆìŒ
* ì‹¤í—˜ ê²°ê³¼ \(Elapsed Time\):
  * TSì˜ ì•Œê³ ë¦¬ì¦˜ ë¡œì§ì€ ê°„ë‹¨í•˜ë‚˜ pymc3 ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ Beta í™•ë¥  ë¶„í¬ ëª¨ë¸ë§ì—ì„œ ë¡œë“œê°€ ë§ì´ ê±¸ë¦¼

### **5.2. numpy ë¼ì´ë¸ŒëŸ¬ë¦¬**

* pymc3 ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•˜ì§€ ì•Šê³  numpyë¡œë§Œ êµ¬í˜„í•œ ì½”ë“œ
* Non-cachingê³¼ cachingìœ¼ë¡œ ë‚˜ëˆ„ì–´ì„œ running time ë¹„êµ \(pymc3 ë¼ì´ë¸ŒëŸ¬ë¦¬ ê¸°ë°˜ êµ¬í˜„ì€ caching ì‚¬ìš©\)
  * Non-cachingì‹œ, rewardê°’ì˜ prior ë¶„í¬ëŠ” ë§¤ time stepë§ˆë‹¤ \[num\_arms x 1\] ë²¡í„°ë¡œ ê³„ì‚°
  * Cachingì‹œ, rewardê°’ì˜ prior ë¶„í¬ëŠ” \[num\_trials x num\_arms\] í–‰ë ¬ë¡œ ë¯¸ë¦¬ ê³„ì‚°í•˜ì—¬ ë§¤ time stepë§ˆë‹¤ \[num\_arms x 1\] ë²¡í„° ë¦¬í„´
* ì‹¤í—˜ ê²°ê³¼, Caching ì‚¬ìš© ì‹œì˜ running timeì´ non-caching ëŒ€ë¹„ ì•½ 3ë°° ì´ìƒ ë” ë¹ ë¦„

```text
bandit = bd.BernoulliBanditNumpy(n_arms)
agents = [
    bd.Agent(bandit, bd.EpsilonGreedyPolicy(0.1), prior=5),
    bd.Agent(bandit, bd.UCBPolicy(1)),
    bd.BetaAgentNumpy(bandit, bd.GreedyPolicy())
]

env = bd.Environment(bandit, agents, label='Epsilon Greedy vs. UCB vs. Thompson Sampling')
scores, optimal = env.run(n_trials, n_experiments)
env.plot_results(scores, optimal)
```

![](../../.gitbook/assets/untitled-3%20%281%29.png)

